{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import erlyx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from erlyx.environment import GymAtariBWEnvironment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from erlyx.policies import PytorchPolicy\n",
    "from erlyx.agents import BaseAgent\n",
    "from erlyx.datasets import SequenceDataset\n",
    "from erlyx.algorithms.ddqn import DoubleDeepQLearner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from erlyx.callbacks import BaseCallback\n",
    "from erlyx.callbacks.recorders import TransitionRecorder, RewardRecorder\n",
    "from erlyx.callbacks.updaters import OnlineUpdater, LinearEpsilonDecay\n",
    "from erlyx.callbacks.checkpoint import PytorchCheckPointer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from datetime import datetime\n",
    "\n",
    "from collections import deque\n",
    "from pathlib import Path\n",
    "import os\n",
    "from time import sleep\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonGreedyHistoryAgent(BaseAgent):\n",
    "    def __init__(self, policy, epsilon, obs_shape):\n",
    "        super(EpsilonGreedyHistoryAgent, self).__init__(policy=policy)\n",
    "        self.epsilon = epsilon\n",
    "        self._obs_shape = obs_shape\n",
    "        self._memory_buffer = deque(maxlen=4)\n",
    "        \n",
    "    def reset_memory(self):\n",
    "        self._memory_buffer = deque([np.zeros(shape=self._obs_shape)]*4, maxlen=4)\n",
    "        \n",
    "    def select_action(self, observation):\n",
    "        self._memory_buffer.append(observation)\n",
    "        observation = np.asarray(list(self._memory_buffer))\n",
    "        if np.random.uniform() < self.epsilon:\n",
    "            return np.random.choice(self.action_space)\n",
    "        distribution = self.policy.get_distribution(observation)\n",
    "        return np.argmax(distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentMemoryReseter(BaseCallback):\n",
    "    def __init__(self, agent):\n",
    "        self._agent = agent\n",
    "        \n",
    "    def on_episode_begin(self, initial_observation):\n",
    "        self._agent.reset_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DuelingNetwork(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DuelingNetwork, self).__init__()\n",
    "        self._conv1 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(4, 32, kernel_size=8, stride=4),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(32, 64, kernel_size=3, stride=2),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            torch.nn.ReLU(),\n",
    "        )\n",
    "        self._linear_advantage = torch.nn.Sequential(\n",
    "            torch.nn.Linear(64*49, 128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(128,  18)\n",
    "        )\n",
    "        self._linear_value = torch.nn.Sequential(\n",
    "            torch.nn.Linear(64*49, 128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(128,  1)\n",
    "        )\n",
    "        \n",
    "                \n",
    "    def forward(self, x):\n",
    "        if len(x.shape) == 3:\n",
    "            x = x.unsqueeze(0)\n",
    "        output = self._conv1(x)\n",
    "        output = output.view(x.shape[0], -1)\n",
    "        advantage = self._linear_advantage(output)\n",
    "        value = self._linear_value(output)\n",
    "        output = value + advantage - advantage.mean()\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNPolicy(PytorchPolicy):\n",
    "    def __init__(self):\n",
    "        self._model = DuelingNetwork()\n",
    "        \n",
    "    @property\n",
    "    def model(self):\n",
    "        return self._model\n",
    "    \n",
    "    def process_observation(self, observation):\n",
    "        tensor = torch.Tensor(observation/128. - 1.)/0.35 # divided by 0.35 because of standard deviation\n",
    "        return tensor\n",
    "        \n",
    "    def get_distribution(self, state):\n",
    "        with torch.no_grad():\n",
    "            self.model.eval()\n",
    "            q_values = self.model(self.process_observation(state))\n",
    "            distribution = torch.nn.functional.softmax(q_values, dim=1)\n",
    "        return distribution.data.cpu().numpy().reshape(-1)\n",
    "    \n",
    "    def num_actions(self):\n",
    "        return 18\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load('20200830213341/latest_checkpoint.sd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "img_hw = (84, 84)\n",
    "\n",
    "# policy\n",
    "policy = CNNPolicy()\n",
    "policy.model.load_state_dict(checkpoint['model'])\n",
    "\n",
    "# agent\n",
    "agent = EpsilonGreedyHistoryAgent(policy=policy, epsilon=0.01, obs_shape=img_hw)\n",
    "\n",
    "# dataset\n",
    "dataset_maxlen = 1_000_000\n",
    "dtypes = {o: np.uint8 for o in ['state', 'action', 'done']}\n",
    "dtypes['reward'] = np.float32\n",
    "dataset = SequenceDataset(input_shape=img_hw, max_length=dataset_maxlen, history_length=4, dtypes=dtypes)\n",
    "\n",
    "# learner\n",
    "learner = DoubleDeepQLearner(32, policy, 1e-4/4, 30_000, loss_func=torch.nn.SmoothL1Loss())\n",
    "learner.optimizer.load_state_dict(checkpoint['optimizer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "hf = h5py.File('20200830213341/dataset.h5', 'r')\n",
    "\n",
    "dataset._actions = np.array(hf.get('actions'))\n",
    "\n",
    "dataset._rewards = np.array(hf.get('rewards'))\n",
    "\n",
    "dataset._states = np.array(hf.get('states'))\n",
    "\n",
    "dataset._dones = np.array(hf.get('dones'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset._position = 8000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logging to folder 20200904195957\n"
     ]
    }
   ],
   "source": [
    "log_folder = Path(datetime.now().strftime(\"%Y%m%d%H%M%S\"))\n",
    "os.mkdir(log_folder)\n",
    "print(f'logging to folder {log_folder}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_callbacks = [\n",
    "    # base\n",
    "    AgentMemoryReseter(agent=agent),\n",
    "    TransitionRecorder(dataset), \n",
    "    OnlineUpdater(learner, dataset, 0, 1),\n",
    "    LinearEpsilonDecay(agent, 0.01, 0.01, 1e-6),\n",
    "    # logging\n",
    "    RewardRecorder(log_folder/'reward_log'),\n",
    "    PytorchCheckPointer(GymAtariBWEnvironment('Seaquest-v0', simplified_reward=False, img_hw=(84,84)), agent, \n",
    "                        learner, 25_000, 0, log_folder, log_folder/'checkpoint_log')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa2eaca4face4fb68f090d39be011a62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=12000000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new best wachin!\n",
      "[Evaluation] mean: 8134.0, rewards: [8390.0, 2280.0, 4720.0, 5340.0, 19940.0]\n",
      "[Evaluation] mean: 7280.0, rewards: [2590.0, 17370.0, 700.0, 13150.0, 2590.0]\n",
      "new best wachin!\n",
      "[Evaluation] mean: 8576.0, rewards: [3090.0, 11430.0, 17900.0, 8060.0, 2400.0]\n",
      "[Evaluation] mean: 8038.0, rewards: [7770.0, 2850.0, 11070.0, 9650.0, 8850.0]\n",
      "[Evaluation] mean: 5262.0, rewards: [2060.0, 3200.0, 8880.0, 6970.0, 5200.0]\n",
      "new best wachin!\n",
      "[Evaluation] mean: 10910.0, rewards: [8500.0, 16020.0, 560.0, 17450.0, 12020.0]\n",
      "new best wachin!\n",
      "[Evaluation] mean: 13658.0, rewards: [14840.0, 14950.0, 14090.0, 15630.0, 8780.0]\n",
      "[Evaluation] mean: 11010.0, rewards: [5420.0, 27990.0, 6600.0, 7260.0, 7780.0]\n",
      "[Evaluation] mean: 5924.0, rewards: [3210.0, 2820.0, 5990.0, 8200.0, 9400.0]\n",
      "[Evaluation] mean: 10276.0, rewards: [7880.0, 7390.0, 7040.0, 21960.0, 7110.0]\n",
      "[Evaluation] mean: 7358.0, rewards: [6520.0, 3070.0, 13020.0, 6770.0, 7410.0]\n",
      "[Evaluation] mean: 11314.0, rewards: [18690.0, 6300.0, 9710.0, 11310.0, 10560.0]\n",
      "[Evaluation] mean: 10028.0, rewards: [3930.0, 9340.0, 2290.0, 17030.0, 17550.0]\n",
      "[Evaluation] mean: 8358.0, rewards: [15400.0, 14450.0, 3610.0, 5430.0, 2900.0]\n",
      "[Evaluation] mean: 10450.0, rewards: [6550.0, 12680.0, 3800.0, 8430.0, 20790.0]\n",
      "[Evaluation] mean: 4690.0, rewards: [2040.0, 2600.0, 6950.0, 6730.0, 5130.0]\n",
      "[Evaluation] mean: 10380.0, rewards: [16520.0, 10750.0, 6740.0, 6550.0, 11340.0]\n",
      "[Evaluation] mean: 10890.0, rewards: [5690.0, 19220.0, 16810.0, 2070.0, 10660.0]\n",
      "[Evaluation] mean: 9408.0, rewards: [3220.0, 26730.0, 4690.0, 8480.0, 3920.0]\n",
      "[Evaluation] mean: 10340.0, rewards: [12120.0, 3350.0, 6030.0, 7910.0, 22290.0]\n",
      "[Evaluation] mean: 8186.0, rewards: [11870.0, 15540.0, 7540.0, 2120.0, 3860.0]\n",
      "[Evaluation] mean: 12520.0, rewards: [12260.0, 17660.0, 8100.0, 9620.0, 14960.0]\n",
      "[Evaluation] mean: 5674.0, rewards: [5600.0, 5090.0, 3230.0, 6500.0, 7950.0]\n",
      "[Evaluation] mean: 6406.0, rewards: [1840.0, 6170.0, 13090.0, 5400.0, 5530.0]\n",
      "[Evaluation] mean: 12500.0, rewards: [5320.0, 20370.0, 11980.0, 17270.0, 7560.0]\n",
      "[Evaluation] mean: 12220.0, rewards: [18820.0, 18120.0, 1750.0, 12790.0, 9620.0]\n",
      "[Evaluation] mean: 9644.0, rewards: [12990.0, 760.0, 9910.0, 5820.0, 18740.0]\n",
      "[Evaluation] mean: 11694.0, rewards: [2900.0, 6690.0, 25180.0, 21500.0, 2200.0]\n",
      "[Evaluation] mean: 11300.0, rewards: [29130.0, 15130.0, 5340.0, 6480.0, 420.0]\n",
      "[Evaluation] mean: 13478.0, rewards: [11930.0, 6200.0, 13860.0, 17250.0, 18150.0]\n",
      "[Evaluation] mean: 8790.0, rewards: [15020.0, 5970.0, 13610.0, 2210.0, 7140.0]\n",
      "[Evaluation] mean: 12002.0, rewards: [2120.0, 6020.0, 31640.0, 15330.0, 4900.0]\n",
      "[Evaluation] mean: 5506.0, rewards: [5100.0, 6220.0, 7990.0, 5170.0, 3050.0]\n",
      "[Evaluation] mean: 4074.0, rewards: [4720.0, 2210.0, 10610.0, 2350.0, 480.0]\n",
      "[Evaluation] mean: 5450.0, rewards: [6210.0, 14810.0, 2760.0, 840.0, 2630.0]\n",
      "[Evaluation] mean: 8862.0, rewards: [3210.0, 6240.0, 11560.0, 5120.0, 18180.0]\n",
      "[Evaluation] mean: 7982.0, rewards: [6290.0, 9660.0, 8940.0, 2430.0, 12590.0]\n",
      "new best wachin!\n",
      "[Evaluation] mean: 13926.0, rewards: [6200.0, 23370.0, 9530.0, 6330.0, 24200.0]\n",
      "[Evaluation] mean: 12172.0, rewards: [860.0, 21000.0, 5790.0, 16110.0, 17100.0]\n",
      "[Evaluation] mean: 8960.0, rewards: [15860.0, 9950.0, 12060.0, 4560.0, 2370.0]\n",
      "[Evaluation] mean: 8382.0, rewards: [15670.0, 14560.0, 6210.0, 3460.0, 2010.0]\n",
      "[Evaluation] mean: 10728.0, rewards: [7100.0, 16940.0, 5280.0, 13390.0, 10930.0]\n",
      "new best wachin!\n",
      "[Evaluation] mean: 14520.0, rewards: [16760.0, 15290.0, 11630.0, 17520.0, 11400.0]\n",
      "[Evaluation] mean: 7608.0, rewards: [4940.0, 4620.0, 9660.0, 7090.0, 11730.0]\n",
      "[Evaluation] mean: 12148.0, rewards: [13220.0, 4070.0, 11630.0, 13930.0, 17890.0]\n",
      "[Evaluation] mean: 9026.0, rewards: [7540.0, 14930.0, 9260.0, 6490.0, 6910.0]\n",
      "[Evaluation] mean: 8104.0, rewards: [9260.0, 13820.0, 5020.0, 9980.0, 2440.0]\n",
      "[Evaluation] mean: 9086.0, rewards: [6760.0, 15710.0, 5220.0, 8720.0, 9020.0]\n",
      "[Evaluation] mean: 9012.0, rewards: [2250.0, 2780.0, 3830.0, 19350.0, 16850.0]\n",
      "[Evaluation] mean: 7930.0, rewards: [4840.0, 580.0, 13330.0, 16440.0, 4460.0]\n",
      "[Evaluation] mean: 12280.0, rewards: [17990.0, 16840.0, 6290.0, 15320.0, 4960.0]\n",
      "[Evaluation] mean: 8858.0, rewards: [4370.0, 14370.0, 8100.0, 7610.0, 9840.0]\n",
      "[Evaluation] mean: 12644.0, rewards: [16930.0, 9030.0, 4310.0, 6660.0, 26290.0]\n",
      "[Evaluation] mean: 9684.0, rewards: [16760.0, 2470.0, 6730.0, 12930.0, 9530.0]\n",
      "[Evaluation] mean: 11602.0, rewards: [13360.0, 5770.0, 6310.0, 15470.0, 17100.0]\n",
      "[Evaluation] mean: 12674.0, rewards: [8020.0, 11910.0, 23550.0, 5690.0, 14200.0]\n",
      "[Evaluation] mean: 6268.0, rewards: [5760.0, 5960.0, 8720.0, 4330.0, 6570.0]\n",
      "new best wachin!\n",
      "[Evaluation] mean: 15376.0, rewards: [7450.0, 16770.0, 7440.0, 25910.0, 19310.0]\n",
      "[Evaluation] mean: 4296.0, rewards: [2070.0, 6740.0, 2220.0, 8000.0, 2450.0]\n",
      "[Evaluation] mean: 13684.0, rewards: [12100.0, 18050.0, 14500.0, 8570.0, 15200.0]\n",
      "[Evaluation] mean: 9024.0, rewards: [8780.0, 16510.0, 12760.0, 3370.0, 3700.0]\n",
      "[Evaluation] mean: 11460.0, rewards: [18930.0, 3950.0, 27110.0, 2920.0, 4390.0]\n",
      "[Evaluation] mean: 10596.0, rewards: [16770.0, 5370.0, 2140.0, 8720.0, 19980.0]\n",
      "[Evaluation] mean: 7352.0, rewards: [10820.0, 2460.0, 6350.0, 14450.0, 2680.0]\n",
      "[Evaluation] mean: 14528.0, rewards: [9750.0, 14940.0, 20680.0, 3010.0, 24260.0]\n",
      "new best wachin!\n",
      "[Evaluation] mean: 17862.0, rewards: [17460.0, 16730.0, 16590.0, 19440.0, 19090.0]\n",
      "[Evaluation] mean: 12962.0, rewards: [16200.0, 14450.0, 4100.0, 13590.0, 16470.0]\n",
      "[Evaluation] mean: 10574.0, rewards: [7450.0, 5270.0, 6680.0, 18020.0, 15450.0]\n",
      "[Evaluation] mean: 8844.0, rewards: [7340.0, 6110.0, 13640.0, 5510.0, 11620.0]\n",
      "[Evaluation] mean: 14156.0, rewards: [13510.0, 8540.0, 15460.0, 5240.0, 28030.0]\n",
      "[Evaluation] mean: 11250.0, rewards: [3450.0, 9990.0, 11010.0, 17840.0, 13960.0]\n",
      "[Evaluation] mean: 14698.0, rewards: [9720.0, 22850.0, 15000.0, 23720.0, 2200.0]\n",
      "[Evaluation] mean: 12684.0, rewards: [8910.0, 6020.0, 18630.0, 22020.0, 7840.0]\n",
      "[Evaluation] mean: 14586.0, rewards: [9910.0, 12110.0, 21700.0, 17980.0, 11230.0]\n",
      "[Evaluation] mean: 10610.0, rewards: [15200.0, 6760.0, 4170.0, 17340.0, 9580.0]\n",
      "new best wachin!\n",
      "[Evaluation] mean: 19794.0, rewards: [13870.0, 22210.0, 19680.0, 15690.0, 27520.0]\n",
      "[Evaluation] mean: 10718.0, rewards: [9630.0, 6810.0, 23730.0, 8240.0, 5180.0]\n",
      "[Evaluation] mean: 14610.0, rewards: [22010.0, 23280.0, 8590.0, 3260.0, 15910.0]\n",
      "[Evaluation] mean: 13040.0, rewards: [9180.0, 1810.0, 13740.0, 18980.0, 21490.0]\n",
      "[Evaluation] mean: 6968.0, rewards: [4700.0, 16180.0, 1810.0, 9760.0, 2390.0]\n",
      "[Evaluation] mean: 9890.0, rewards: [9010.0, 2790.0, 23350.0, 10120.0, 4180.0]\n",
      "[Evaluation] mean: 10794.0, rewards: [18820.0, 17600.0, 7540.0, 7500.0, 2510.0]\n",
      "[Evaluation] mean: 10064.0, rewards: [15610.0, 1630.0, 3390.0, 3000.0, 26690.0]\n",
      "[Evaluation] mean: 7270.0, rewards: [7820.0, 3960.0, 3780.0, 14480.0, 6310.0]\n",
      "[Evaluation] mean: 11926.0, rewards: [16510.0, 21430.0, 3450.0, 11130.0, 7110.0]\n",
      "[Evaluation] mean: 13472.0, rewards: [25820.0, 14030.0, 11750.0, 8740.0, 7020.0]\n",
      "[Evaluation] mean: 11746.0, rewards: [5390.0, 8320.0, 11180.0, 21920.0, 11920.0]\n",
      "[Evaluation] mean: 15080.0, rewards: [16450.0, 19080.0, 6880.0, 16640.0, 16350.0]\n",
      "[Evaluation] mean: 14948.0, rewards: [18610.0, 16820.0, 7060.0, 21030.0, 11220.0]\n",
      "[Evaluation] mean: 12740.0, rewards: [23040.0, 3650.0, 19320.0, 2100.0, 15590.0]\n",
      "[Evaluation] mean: 11660.0, rewards: [31800.0, 7910.0, 14650.0, 1680.0, 2260.0]\n",
      "[Evaluation] mean: 11612.0, rewards: [2930.0, 6840.0, 25010.0, 18290.0, 4990.0]\n",
      "[Evaluation] mean: 17184.0, rewards: [8180.0, 9960.0, 21860.0, 16610.0, 29310.0]\n",
      "[Evaluation] mean: 10976.0, rewards: [16610.0, 6300.0, 18020.0, 8960.0, 4990.0]\n",
      "[Evaluation] mean: 14300.0, rewards: [16840.0, 15870.0, 4250.0, 3350.0, 31190.0]\n",
      "[Evaluation] mean: 10448.0, rewards: [5240.0, 8080.0, 6690.0, 25640.0, 6590.0]\n",
      "[Evaluation] mean: 10906.0, rewards: [9760.0, 6030.0, 8060.0, 27300.0, 3380.0]\n",
      "[Evaluation] mean: 12142.0, rewards: [6190.0, 17710.0, 13450.0, 18370.0, 4990.0]\n",
      "[Evaluation] mean: 11444.0, rewards: [12480.0, 11840.0, 3760.0, 13420.0, 15720.0]\n",
      "[Evaluation] mean: 14930.0, rewards: [18600.0, 23440.0, 8420.0, 18520.0, 5670.0]\n",
      "[Evaluation] mean: 9340.0, rewards: [11280.0, 12820.0, 3330.0, 9970.0, 9300.0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Evaluation] mean: 15212.0, rewards: [24540.0, 28990.0, 5190.0, 3120.0, 14220.0]\n",
      "[Evaluation] mean: 16148.0, rewards: [36090.0, 13820.0, 8120.0, 7210.0, 15500.0]\n",
      "new best wachin!\n",
      "[Evaluation] mean: 27346.0, rewards: [18160.0, 39160.0, 25070.0, 28840.0, 25500.0]\n",
      "[Evaluation] mean: 8666.0, rewards: [14900.0, 15860.0, 7900.0, 2460.0, 2210.0]\n",
      "[Evaluation] mean: 8086.0, rewards: [13670.0, 5420.0, 6240.0, 7210.0, 7890.0]\n",
      "[Evaluation] mean: 12578.0, rewards: [5470.0, 5630.0, 16480.0, 17210.0, 18100.0]\n",
      "[Evaluation] mean: 9668.0, rewards: [12580.0, 6660.0, 15690.0, 920.0, 12490.0]\n",
      "[Evaluation] mean: 17772.0, rewards: [7660.0, 15490.0, 30200.0, 24210.0, 11300.0]\n",
      "[Evaluation] mean: 9112.0, rewards: [6220.0, 12910.0, 4270.0, 5550.0, 16610.0]\n",
      "[Evaluation] mean: 18944.0, rewards: [5770.0, 28670.0, 23420.0, 16730.0, 20130.0]\n",
      "[Evaluation] mean: 9760.0, rewards: [9670.0, 15030.0, 12440.0, 6340.0, 5320.0]\n",
      "[Evaluation] mean: 4054.0, rewards: [2430.0, 1820.0, 6110.0, 6320.0, 3590.0]\n",
      "[Evaluation] mean: 9078.0, rewards: [2750.0, 11960.0, 2730.0, 23760.0, 4190.0]\n",
      "[Evaluation] mean: 9440.0, rewards: [8000.0, 15750.0, 6030.0, 14250.0, 3170.0]\n",
      "[Evaluation] mean: 14174.0, rewards: [25700.0, 23780.0, 9630.0, 2280.0, 9480.0]\n",
      "[Evaluation] mean: 11520.0, rewards: [17950.0, 15800.0, 8110.0, 10970.0, 4770.0]\n",
      "[Evaluation] mean: 18004.0, rewards: [32420.0, 13080.0, 14530.0, 15280.0, 14710.0]\n",
      "[Evaluation] mean: 9688.0, rewards: [7220.0, 23580.0, 8250.0, 7110.0, 2280.0]\n",
      "[Evaluation] mean: 7322.0, rewards: [5130.0, 6440.0, 8250.0, 9860.0, 6930.0]\n",
      "[Evaluation] mean: 8382.0, rewards: [2830.0, 4560.0, 11410.0, 15690.0, 7420.0]\n",
      "[Evaluation] mean: 8268.0, rewards: [14360.0, 6090.0, 2710.0, 7110.0, 11070.0]\n",
      "[Evaluation] mean: 16952.0, rewards: [17470.0, 33190.0, 2780.0, 18230.0, 13090.0]\n",
      "[Evaluation] mean: 9914.0, rewards: [18750.0, 7540.0, 920.0, 16690.0, 5670.0]\n",
      "[Evaluation] mean: 14938.0, rewards: [9220.0, 14110.0, 26740.0, 2480.0, 22140.0]\n",
      "[Evaluation] mean: 17410.0, rewards: [14630.0, 15280.0, 22590.0, 25880.0, 8670.0]\n",
      "[Evaluation] mean: 11066.0, rewards: [6410.0, 15600.0, 8890.0, 8640.0, 15790.0]\n",
      "[Evaluation] mean: 15348.0, rewards: [28280.0, 15540.0, 7560.0, 16870.0, 8490.0]\n",
      "[Evaluation] mean: 8316.0, rewards: [20730.0, 7200.0, 5860.0, 3620.0, 4170.0]\n",
      "[Evaluation] mean: 10786.0, rewards: [18350.0, 9040.0, 3990.0, 20200.0, 2350.0]\n",
      "[Evaluation] mean: 11730.0, rewards: [32820.0, 3640.0, 3140.0, 6690.0, 12360.0]\n",
      "[Evaluation] mean: 7582.0, rewards: [7510.0, 3250.0, 9940.0, 12070.0, 5140.0]\n",
      "[Evaluation] mean: 7970.0, rewards: [7350.0, 4870.0, 23050.0, 2440.0, 2140.0]\n",
      "[Evaluation] mean: 10138.0, rewards: [12090.0, 16220.0, 14360.0, 5470.0, 2550.0]\n",
      "[Evaluation] mean: 15754.0, rewards: [22630.0, 25320.0, 2980.0, 6070.0, 21770.0]\n",
      "[Evaluation] mean: 9834.0, rewards: [6340.0, 16770.0, 15210.0, 4430.0, 6420.0]\n",
      "[Evaluation] mean: 7624.0, rewards: [19150.0, 2820.0, 5340.0, 6290.0, 4520.0]\n",
      "[Evaluation] mean: 9688.0, rewards: [9500.0, 17540.0, 7200.0, 7770.0, 6430.0]\n",
      "[Evaluation] mean: 10190.0, rewards: [4820.0, 18190.0, 12580.0, 10650.0, 4710.0]\n",
      "[Evaluation] mean: 15612.0, rewards: [13820.0, 21170.0, 19140.0, 23590.0, 340.0]\n",
      "[Evaluation] mean: 12282.0, rewards: [14900.0, 18080.0, 9320.0, 16260.0, 2850.0]\n",
      "[Evaluation] mean: 7502.0, rewards: [2910.0, 3400.0, 13270.0, 4690.0, 13240.0]\n",
      "[Evaluation] mean: 7952.0, rewards: [8620.0, 3520.0, 12220.0, 7070.0, 8330.0]\n",
      "[Evaluation] mean: 12956.0, rewards: [24790.0, 1140.0, 8030.0, 25130.0, 5690.0]\n",
      "[Evaluation] mean: 16908.0, rewards: [15650.0, 8410.0, 32190.0, 9110.0, 19180.0]\n",
      "[Evaluation] mean: 12100.0, rewards: [5740.0, 9470.0, 21170.0, 16850.0, 7270.0]\n",
      "[Evaluation] mean: 18254.0, rewards: [16350.0, 18030.0, 17990.0, 32620.0, 6280.0]\n",
      "[Evaluation] mean: 13554.0, rewards: [12550.0, 840.0, 25260.0, 13730.0, 15390.0]\n",
      "[Evaluation] mean: 17722.0, rewards: [13010.0, 24050.0, 11610.0, 21370.0, 18570.0]\n",
      "[Evaluation] mean: 13228.0, rewards: [3840.0, 5640.0, 13050.0, 19170.0, 24440.0]\n",
      "[Evaluation] mean: 12286.0, rewards: [18010.0, 860.0, 4670.0, 15550.0, 22340.0]\n",
      "[Evaluation] mean: 11752.0, rewards: [18240.0, 15080.0, 6730.0, 10930.0, 7780.0]\n",
      "[Evaluation] mean: 12596.0, rewards: [13070.0, 17220.0, 9780.0, 7790.0, 15120.0]\n",
      "[Evaluation] mean: 10614.0, rewards: [15380.0, 5550.0, 3770.0, 25600.0, 2770.0]\n",
      "[Evaluation] mean: 8336.0, rewards: [16130.0, 12780.0, 4400.0, 2950.0, 5420.0]\n",
      "[Evaluation] mean: 10238.0, rewards: [5720.0, 15860.0, 8740.0, 5100.0, 15770.0]\n",
      "[Evaluation] mean: 16784.0, rewards: [27120.0, 16510.0, 17340.0, 16230.0, 6720.0]\n",
      "[Evaluation] mean: 11742.0, rewards: [2400.0, 13540.0, 9110.0, 12200.0, 21460.0]\n",
      "[Evaluation] mean: 7138.0, rewards: [4330.0, 7820.0, 5200.0, 10740.0, 7600.0]\n",
      "[Evaluation] mean: 13536.0, rewards: [16820.0, 23600.0, 9700.0, 7750.0, 9810.0]\n",
      "[Evaluation] mean: 10948.0, rewards: [6880.0, 19810.0, 15220.0, 8990.0, 3840.0]\n",
      "[Evaluation] mean: 9832.0, rewards: [23920.0, 6650.0, 4220.0, 8660.0, 5710.0]\n",
      "[Evaluation] mean: 10090.0, rewards: [5800.0, 15250.0, 5490.0, 14080.0, 9830.0]\n",
      "[Evaluation] mean: 13300.0, rewards: [12270.0, 8730.0, 19350.0, 7500.0, 18650.0]\n",
      "[Evaluation] mean: 14078.0, rewards: [9660.0, 7920.0, 16470.0, 29710.0, 6630.0]\n",
      "[Evaluation] mean: 6758.0, rewards: [520.0, 2670.0, 24740.0, 2950.0, 2910.0]\n",
      "[Evaluation] mean: 3712.0, rewards: [6260.0, 6320.0, 2490.0, 2310.0, 1180.0]\n",
      "[Evaluation] mean: 8462.0, rewards: [620.0, 19600.0, 7860.0, 8300.0, 5930.0]\n",
      "[Evaluation] mean: 14206.0, rewards: [14630.0, 17850.0, 2170.0, 17120.0, 19260.0]\n",
      "[Evaluation] mean: 10908.0, rewards: [9590.0, 19470.0, 19940.0, 3400.0, 2140.0]\n",
      "[Evaluation] mean: 6794.0, rewards: [1200.0, 9930.0, 6540.0, 6630.0, 9670.0]\n",
      "[Evaluation] mean: 12420.0, rewards: [8920.0, 10730.0, 24710.0, 17300.0, 440.0]\n",
      "[Evaluation] mean: 10970.0, rewards: [16900.0, 8240.0, 15800.0, 4220.0, 9690.0]\n",
      "[Evaluation] mean: 7320.0, rewards: [7360.0, 5370.0, 6700.0, 14080.0, 3090.0]\n",
      "[Evaluation] mean: 9848.0, rewards: [5700.0, 13290.0, 7150.0, 14750.0, 8350.0]\n",
      "[Evaluation] mean: 6080.0, rewards: [6020.0, 2000.0, 16390.0, 4130.0, 1860.0]\n",
      "[Evaluation] mean: 9194.0, rewards: [16300.0, 16530.0, 3360.0, 5670.0, 4110.0]\n",
      "[Evaluation] mean: 11866.0, rewards: [22010.0, 17520.0, 4130.0, 10680.0, 4990.0]\n",
      "[Evaluation] mean: 11244.0, rewards: [9440.0, 26390.0, 7410.0, 3850.0, 9130.0]\n",
      "[Evaluation] mean: 11000.0, rewards: [9460.0, 7050.0, 6040.0, 8860.0, 23590.0]\n",
      "[Evaluation] mean: 8238.0, rewards: [5070.0, 13440.0, 7680.0, 8610.0, 6390.0]\n",
      "[Evaluation] mean: 10000.0, rewards: [2420.0, 8250.0, 4590.0, 17470.0, 17270.0]\n",
      "[Evaluation] mean: 11522.0, rewards: [1060.0, 14020.0, 4680.0, 4500.0, 33350.0]\n",
      "[Evaluation] mean: 10640.0, rewards: [3690.0, 21120.0, 13910.0, 5080.0, 9400.0]\n",
      "[Evaluation] mean: 6090.0, rewards: [600.0, 5350.0, 2620.0, 13580.0, 8300.0]\n",
      "[Evaluation] mean: 11912.0, rewards: [26980.0, 15980.0, 7410.0, 4140.0, 5050.0]\n",
      "[Evaluation] mean: 11612.0, rewards: [26890.0, 7210.0, 880.0, 9760.0, 13320.0]\n",
      "[Evaluation] mean: 13088.0, rewards: [12280.0, 7220.0, 7010.0, 16460.0, 22470.0]\n",
      "[Evaluation] mean: 10076.0, rewards: [12590.0, 9820.0, 9700.0, 3200.0, 15070.0]\n",
      "[Evaluation] mean: 11262.0, rewards: [5360.0, 12780.0, 3660.0, 11320.0, 23190.0]\n",
      "[Evaluation] mean: 13514.0, rewards: [16400.0, 15640.0, 600.0, 15080.0, 19850.0]\n",
      "[Evaluation] mean: 11166.0, rewards: [13330.0, 15240.0, 3540.0, 18650.0, 5070.0]\n",
      "[Evaluation] mean: 13860.0, rewards: [8390.0, 18550.0, 9590.0, 14080.0, 18690.0]\n",
      "[Evaluation] mean: 9164.0, rewards: [3970.0, 8150.0, 23140.0, 2210.0, 8350.0]\n",
      "[Evaluation] mean: 8288.0, rewards: [3450.0, 12590.0, 2620.0, 7450.0, 15330.0]\n",
      "[Evaluation] mean: 12870.0, rewards: [7280.0, 18510.0, 14740.0, 15350.0, 8470.0]\n",
      "[Evaluation] mean: 12650.0, rewards: [27360.0, 13040.0, 4150.0, 6520.0, 12180.0]\n",
      "[Evaluation] mean: 11676.0, rewards: [22880.0, 15830.0, 11670.0, 4060.0, 3940.0]\n",
      "[Evaluation] mean: 12948.0, rewards: [4470.0, 18160.0, 8610.0, 2810.0, 30690.0]\n",
      "[Evaluation] mean: 12956.0, rewards: [8890.0, 2780.0, 25930.0, 23750.0, 3430.0]\n",
      "[Evaluation] mean: 13392.0, rewards: [15560.0, 23660.0, 9170.0, 3290.0, 15280.0]\n",
      "[Evaluation] mean: 15658.0, rewards: [10790.0, 26180.0, 15880.0, 17090.0, 8350.0]\n",
      "[Evaluation] mean: 15158.0, rewards: [21710.0, 14670.0, 16930.0, 15840.0, 6640.0]\n",
      "[Evaluation] mean: 14520.0, rewards: [15200.0, 15820.0, 20780.0, 3460.0, 17340.0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Evaluation] mean: 10478.0, rewards: [20630.0, 12960.0, 4360.0, 5830.0, 8610.0]\n",
      "[Evaluation] mean: 13880.0, rewards: [15380.0, 9780.0, 21380.0, 13670.0, 9190.0]\n",
      "[Evaluation] mean: 11880.0, rewards: [4800.0, 8130.0, 14230.0, 12290.0, 19950.0]\n",
      "[Evaluation] mean: 12850.0, rewards: [22850.0, 16130.0, 2310.0, 6150.0, 16810.0]\n",
      "[Evaluation] mean: 6954.0, rewards: [4190.0, 3000.0, 6680.0, 15540.0, 5360.0]\n",
      "[Evaluation] mean: 9624.0, rewards: [4650.0, 12770.0, 7380.0, 17890.0, 5430.0]\n",
      "[Evaluation] mean: 8474.0, rewards: [16610.0, 2150.0, 14500.0, 3020.0, 6090.0]\n",
      "[Evaluation] mean: 9610.0, rewards: [11330.0, 14580.0, 6570.0, 12980.0, 2590.0]\n",
      "[Evaluation] mean: 11138.0, rewards: [4850.0, 15350.0, 16890.0, 15800.0, 2800.0]\n",
      "[Evaluation] mean: 9742.0, rewards: [16890.0, 7690.0, 5690.0, 3730.0, 14710.0]\n",
      "[Evaluation] mean: 16648.0, rewards: [2490.0, 29610.0, 15800.0, 17250.0, 18090.0]\n",
      "[Evaluation] mean: 14248.0, rewards: [26270.0, 9970.0, 15520.0, 5680.0, 13800.0]\n",
      "[Evaluation] mean: 13782.0, rewards: [27410.0, 3410.0, 7940.0, 12920.0, 17230.0]\n",
      "[Evaluation] mean: 12188.0, rewards: [2380.0, 23110.0, 2990.0, 22810.0, 9650.0]\n",
      "[Evaluation] mean: 11114.0, rewards: [16300.0, 6810.0, 12950.0, 5640.0, 13870.0]\n",
      "[Evaluation] mean: 6564.0, rewards: [2770.0, 7360.0, 6660.0, 580.0, 15450.0]\n",
      "[Evaluation] mean: 7386.0, rewards: [3380.0, 5080.0, 9550.0, 10500.0, 8420.0]\n",
      "[Evaluation] mean: 12548.0, rewards: [13400.0, 9570.0, 20740.0, 10700.0, 8330.0]\n",
      "[Evaluation] mean: 14698.0, rewards: [29220.0, 9570.0, 7190.0, 19730.0, 7780.0]\n",
      "[Evaluation] mean: 11770.0, rewards: [23340.0, 13740.0, 16120.0, 3890.0, 1760.0]\n",
      "[Evaluation] mean: 5084.0, rewards: [9290.0, 3330.0, 2750.0, 7600.0, 2450.0]\n",
      "[Evaluation] mean: 11514.0, rewards: [3310.0, 13440.0, 18880.0, 7360.0, 14580.0]\n",
      "[Evaluation] mean: 5402.0, rewards: [4520.0, 6520.0, 2780.0, 9480.0, 3710.0]\n",
      "[Evaluation] mean: 6600.0, rewards: [4490.0, 14320.0, 2210.0, 2610.0, 9370.0]\n",
      "[Evaluation] mean: 12100.0, rewards: [3590.0, 5020.0, 23480.0, 19690.0, 8720.0]\n",
      "[Evaluation] mean: 10088.0, rewards: [9340.0, 13300.0, 19720.0, 3330.0, 4750.0]\n",
      "[Evaluation] mean: 6218.0, rewards: [11990.0, 4060.0, 10120.0, 1590.0, 3330.0]\n",
      "[Evaluation] mean: 13312.0, rewards: [22650.0, 16230.0, 4960.0, 17440.0, 5280.0]\n",
      "[Evaluation] mean: 4880.0, rewards: [7780.0, 400.0, 13010.0, 1140.0, 2070.0]\n",
      "[Evaluation] mean: 10600.0, rewards: [7650.0, 16020.0, 6480.0, 13010.0, 9840.0]\n",
      "[Evaluation] mean: 18706.0, rewards: [14700.0, 31400.0, 9570.0, 17900.0, 19960.0]\n",
      "[Evaluation] mean: 18026.0, rewards: [27550.0, 2440.0, 8410.0, 22400.0, 29330.0]\n",
      "[Evaluation] mean: 19728.0, rewards: [8010.0, 33590.0, 18400.0, 19160.0, 19480.0]\n",
      "[Evaluation] mean: 14516.0, rewards: [18950.0, 8760.0, 19010.0, 13080.0, 12780.0]\n",
      "[Evaluation] mean: 17510.0, rewards: [4410.0, 13370.0, 9280.0, 31350.0, 29140.0]\n",
      "[Evaluation] mean: 14364.0, rewards: [12060.0, 17580.0, 13530.0, 17000.0, 11650.0]\n",
      "[Evaluation] mean: 9600.0, rewards: [2870.0, 2330.0, 19360.0, 7210.0, 16230.0]\n",
      "[Evaluation] mean: 10854.0, rewards: [12560.0, 12350.0, 6870.0, 9640.0, 12850.0]\n",
      "[Evaluation] mean: 17402.0, rewards: [25940.0, 25090.0, 8000.0, 9290.0, 18690.0]\n",
      "[Evaluation] mean: 16346.0, rewards: [14520.0, 14010.0, 16530.0, 13830.0, 22840.0]\n",
      "[Evaluation] mean: 6852.0, rewards: [3770.0, 7290.0, 3990.0, 5650.0, 13560.0]\n",
      "[Evaluation] mean: 10556.0, rewards: [10730.0, 8580.0, 11560.0, 6940.0, 14970.0]\n",
      "[Evaluation] mean: 6972.0, rewards: [8240.0, 3560.0, 6730.0, 5200.0, 11130.0]\n",
      "[Evaluation] mean: 6832.0, rewards: [7760.0, 14380.0, 1800.0, 6940.0, 3280.0]\n",
      "[Evaluation] mean: 4804.0, rewards: [5350.0, 3600.0, 3480.0, 4890.0, 6700.0]\n",
      "[Evaluation] mean: 16018.0, rewards: [10770.0, 12520.0, 16340.0, 16220.0, 24240.0]\n",
      "[Evaluation] mean: 9068.0, rewards: [6890.0, 2180.0, 7910.0, 11830.0, 16530.0]\n",
      "[Evaluation] mean: 6162.0, rewards: [2860.0, 7760.0, 2900.0, 7860.0, 9430.0]\n",
      "[Evaluation] mean: 9718.0, rewards: [1990.0, 17620.0, 4420.0, 18590.0, 5970.0]\n",
      "[Evaluation] mean: 7106.0, rewards: [16990.0, 3210.0, 6460.0, 1160.0, 7710.0]\n",
      "[Evaluation] mean: 21208.0, rewards: [46410.0, 18030.0, 18010.0, 15910.0, 7680.0]\n",
      "[Evaluation] mean: 11660.0, rewards: [14850.0, 6540.0, 13500.0, 14010.0, 9400.0]\n",
      "[Evaluation] mean: 6608.0, rewards: [14420.0, 9630.0, 4970.0, 2400.0, 1620.0]\n",
      "[Evaluation] mean: 15288.0, rewards: [12650.0, 23000.0, 9870.0, 20970.0, 9950.0]\n",
      "[Evaluation] mean: 13866.0, rewards: [12780.0, 9380.0, 17740.0, 20920.0, 8510.0]\n",
      "[Evaluation] mean: 4732.0, rewards: [8850.0, 4020.0, 2260.0, 2560.0, 5970.0]\n",
      "[Evaluation] mean: 13384.0, rewards: [15220.0, 20900.0, 20880.0, 6500.0, 3420.0]\n",
      "[Evaluation] mean: 10892.0, rewards: [17090.0, 8330.0, 6390.0, 13510.0, 9140.0]\n",
      "[Evaluation] mean: 4940.0, rewards: [4460.0, 5130.0, 6960.0, 3040.0, 5110.0]\n",
      "[Evaluation] mean: 16402.0, rewards: [18630.0, 19030.0, 16700.0, 12780.0, 14870.0]\n",
      "[Evaluation] mean: 14584.0, rewards: [16340.0, 12100.0, 17110.0, 8540.0, 18830.0]\n",
      "[Evaluation] mean: 7776.0, rewards: [9480.0, 12310.0, 5250.0, 9030.0, 2810.0]\n",
      "[Evaluation] mean: 16986.0, rewards: [24200.0, 12020.0, 26490.0, 6500.0, 15720.0]\n",
      "[Evaluation] mean: 12204.0, rewards: [18000.0, 11160.0, 4330.0, 5750.0, 21780.0]\n",
      "[Evaluation] mean: 17704.0, rewards: [26610.0, 30800.0, 4680.0, 10730.0, 15700.0]\n",
      "[Evaluation] mean: 10826.0, rewards: [15680.0, 8650.0, 7850.0, 6440.0, 15510.0]\n",
      "[Evaluation] mean: 8320.0, rewards: [9140.0, 3940.0, 18010.0, 5250.0, 5260.0]\n",
      "[Evaluation] mean: 10556.0, rewards: [15880.0, 15560.0, 3250.0, 4470.0, 13620.0]\n",
      "[Evaluation] mean: 13646.0, rewards: [13000.0, 6510.0, 12190.0, 27380.0, 9150.0]\n",
      "[Evaluation] mean: 12724.0, rewards: [8530.0, 6320.0, 8450.0, 18730.0, 21590.0]\n",
      "[Evaluation] mean: 17056.0, rewards: [6800.0, 23710.0, 22320.0, 8820.0, 23630.0]\n",
      "[Evaluation] mean: 13888.0, rewards: [9120.0, 9590.0, 24860.0, 9090.0, 16780.0]\n",
      "[Evaluation] mean: 10820.0, rewards: [3420.0, 9130.0, 15290.0, 2830.0, 23430.0]\n",
      "[Evaluation] mean: 11764.0, rewards: [3070.0, 17620.0, 8580.0, 20800.0, 8750.0]\n",
      "[Evaluation] mean: 13258.0, rewards: [9970.0, 13400.0, 14520.0, 8460.0, 19940.0]\n",
      "[Evaluation] mean: 8748.0, rewards: [9100.0, 7190.0, 7410.0, 8550.0, 11490.0]\n",
      "[Evaluation] mean: 13368.0, rewards: [2120.0, 19880.0, 3020.0, 14280.0, 27540.0]\n",
      "[Evaluation] mean: 5746.0, rewards: [5370.0, 4280.0, 9610.0, 5760.0, 3710.0]\n",
      "[Evaluation] mean: 13848.0, rewards: [15050.0, 12000.0, 8890.0, 17270.0, 16030.0]\n",
      "[Evaluation] mean: 9514.0, rewards: [4350.0, 5790.0, 16420.0, 7560.0, 13450.0]\n",
      "[Evaluation] mean: 12126.0, rewards: [14630.0, 12750.0, 18270.0, 8690.0, 6290.0]\n",
      "[Evaluation] mean: 7156.0, rewards: [4370.0, 7610.0, 3080.0, 11650.0, 9070.0]\n",
      "[Evaluation] mean: 14054.0, rewards: [17220.0, 8000.0, 5160.0, 9910.0, 29980.0]\n",
      "[Evaluation] mean: 9650.0, rewards: [2900.0, 9330.0, 13760.0, 13540.0, 8720.0]\n",
      "[Evaluation] mean: 9280.0, rewards: [2010.0, 2660.0, 3610.0, 5740.0, 32380.0]\n",
      "[Evaluation] mean: 6400.0, rewards: [2920.0, 8290.0, 11410.0, 6170.0, 3210.0]\n",
      "[Evaluation] mean: 10962.0, rewards: [13590.0, 6330.0, 11330.0, 13840.0, 9720.0]\n",
      "[Evaluation] mean: 10316.0, rewards: [2530.0, 4740.0, 22070.0, 15470.0, 6770.0]\n",
      "[Evaluation] mean: 11202.0, rewards: [2050.0, 3410.0, 26190.0, 8220.0, 16140.0]\n",
      "[Evaluation] mean: 13654.0, rewards: [3710.0, 17290.0, 26090.0, 17560.0, 3620.0]\n",
      "[Evaluation] mean: 9210.0, rewards: [18660.0, 17450.0, 2640.0, 2840.0, 4460.0]\n",
      "[Evaluation] mean: 10228.0, rewards: [22520.0, 2610.0, 9610.0, 7540.0, 8860.0]\n",
      "[Evaluation] mean: 9074.0, rewards: [2680.0, 6060.0, 23690.0, 4530.0, 8410.0]\n",
      "[Evaluation] mean: 5976.0, rewards: [3410.0, 2420.0, 7010.0, 8240.0, 8800.0]\n",
      "[Evaluation] mean: 16102.0, rewards: [18830.0, 18520.0, 14950.0, 22350.0, 5860.0]\n",
      "[Evaluation] mean: 9422.0, rewards: [7530.0, 9460.0, 5440.0, 5650.0, 19030.0]\n",
      "[Evaluation] mean: 16256.0, rewards: [28520.0, 9630.0, 24030.0, 14840.0, 4260.0]\n",
      "[Evaluation] mean: 14762.0, rewards: [1860.0, 2740.0, 7600.0, 36990.0, 24620.0]\n",
      "[Evaluation] mean: 19356.0, rewards: [3150.0, 22440.0, 33950.0, 11010.0, 26230.0]\n",
      "[Evaluation] mean: 9022.0, rewards: [9460.0, 2800.0, 22530.0, 6700.0, 3620.0]\n",
      "[Evaluation] mean: 7616.0, rewards: [18600.0, 2770.0, 2680.0, 6770.0, 7260.0]\n",
      "[Evaluation] mean: 11024.0, rewards: [12010.0, 13270.0, 6340.0, 14770.0, 8730.0]\n",
      "[Evaluation] mean: 7244.0, rewards: [16360.0, 9760.0, 2180.0, 5100.0, 2820.0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Evaluation] mean: 8814.0, rewards: [7680.0, 17350.0, 2240.0, 8690.0, 8110.0]\n",
      "[Evaluation] mean: 13658.0, rewards: [18240.0, 4750.0, 16220.0, 17690.0, 11390.0]\n",
      "[Evaluation] mean: 5834.0, rewards: [2390.0, 2000.0, 2230.0, 4570.0, 17980.0]\n",
      "[Evaluation] mean: 8814.0, rewards: [2900.0, 12250.0, 12950.0, 13190.0, 2780.0]\n",
      "[Evaluation] mean: 8824.0, rewards: [2150.0, 2920.0, 13180.0, 5660.0, 20210.0]\n",
      "[Evaluation] mean: 16942.0, rewards: [12880.0, 38360.0, 16360.0, 4610.0, 12500.0]\n",
      "[Evaluation] mean: 5432.0, rewards: [2770.0, 420.0, 380.0, 7860.0, 15730.0]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-044cec739c37>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0merlyx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_steps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGymAtariBWEnvironment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Seaquest-v0'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_hw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m84\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m84\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m12_000_000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_callbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/rl/lib/python3.6/site-packages/erlyx-0.0.1-py3.6.egg/erlyx/run.py\u001b[0m in \u001b[0;36mrun_steps\u001b[0;34m(environment, agent, n_steps, callbacks, use_tqdm)\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepisode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'on_step_end'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m             \u001b[0mepisode_counter\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0muse_tqdm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rl/lib/python3.6/site-packages/erlyx-0.0.1-py3.6.egg/erlyx/callbacks/base.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, event, **params)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rl/lib/python3.6/site-packages/erlyx-0.0.1-py3.6.egg/erlyx/callbacks/base.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, event, **params)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mBaseCallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rl/lib/python3.6/site-packages/erlyx-0.0.1-py3.6.egg/erlyx/callbacks/updaters.py\u001b[0m in \u001b[0;36mon_step_end\u001b[0;34m(self, action, observation, reward, done)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_observations\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_frequency\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_learner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rl/lib/python3.6/site-packages/erlyx-0.0.1-py3.6.egg/erlyx/algorithms/ddqn.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;31m#torch.nn.utils.clip_grad_norm_(self.policy.model.parameters(), 10)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m# Update target policy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rl/lib/python3.6/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rl/lib/python3.6/site-packages/torch/optim/adamw.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m                 \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m                 \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "erlyx.run_steps(GymAtariBWEnvironment('Seaquest-v0', img_hw=(84,84)), agent, 12_000_000, train_callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persist dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf = h5py.File(log_folder/'dataset.h5', 'w')\n",
    "hf.create_dataset('states', data=dataset._states, dtype=np.uint8)\n",
    "hf.create_dataset('actions', data=dataset._actions, dtype=np.uint8)\n",
    "hf.create_dataset('rewards', data=dataset._rewards, dtype=np.float32)\n",
    "hf.create_dataset('dones', data=dataset._dones, dtype=np.uint8)\n",
    "print(f'latest dataset position: {dataset._position}')\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "position = 816565"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "816565"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset._position"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7e25a209770483fb13aa6befbbfea33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "env = GymAtariBWEnvironment('Seaquest-v0', repeat=4, simplified_reward=False, img_hw=(84, 84))\n",
    "rr = RewardRecorder()\n",
    "tr = Tracker()\n",
    "erlyx.run_episodes(env, agent, 1, [AgentMemoryReseter(agent), rr, tr])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
